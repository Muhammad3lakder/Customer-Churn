---
title: "Why My Customers Are Leaving?"
author: "Muhammad Fawzi Al-Akhdar"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.pos = "H",
  fig.align = 'center',
  cache = TRUE
)
```


```{r message=FALSE, warning=FALSE, include=F}
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(infer)
library(skimr)
library(stringr)
library(caret)
library(broom)
```


# Abstract

This research examines the factors that lead to customer churn in the telecommunications industry, with a case study of a telecommunications company. Using exploratory data analysis and predictive modeling, key variables such as SMS usage, calling plan, and customer age group are examined for their impact on churn. Logistic regression modeling with cross-validation provides insight into the accuracy of churn prediction and guides targeted retention strategies. Limitations and future enhancements to improve model accuracy and practical application are discussed.



# Introduction

Customer attrition, or churn, is a critical challenge for businesses, especially in competitive industries like telecommunications.[**"Becuase retaining existing customers is more profitable than acquiring new customers, primarily due to savings in acquisition costs, higher volume of service consumption, and customer referrals."**](https://en.wikipedia.org/wiki/Customer_attrition#Research)

For a telecom company based in Iran, building an effective customer retention program can reduce churn.To do so they can use their [**Dataset**](https://doi.org/10.24432/C5JW3Z), to uncover patterns to understand why customers leave and identify those at high risk of leaving by accurately predicting customer churn so they can target them. By carefully analyzing and digging deeper into the dataset, we can predict and understand customer churn.

   
# Methodology

## Data Cleaning

Consistency of the dataset is important for meaningful data analysis, and we will ensure it.

Our steps include:
1. Rename columns for readability.
2. Convert binary and categorical variables to factors.
3. Check for missing values for data completeness.


```{r, include=F}
# load the data set
customers <- read.csv("Customer Churn.csv")
```



Here is a snapshot of the original dataset:



```{r echo=FALSE}
first_snapshot <- customers |>
  sample_n(5)

kable(
  first_snapshot |>
    select(1:6),
  caption = "A snapshot of the orginal dataset"
)
```




Here is a snapshot of the dataset after cleaning:

```{r echo=FALSE}
# Cleaning up the data

# Making sure the names are unique and consistent
customers <- customers |>
  clean_names()

# Changing binary variables into categorical variables
customers$age_group <- factor(customers$age_group)

customers$churn <- factor(customers$churn, levels = c(0, 1), labels = c("non-churn", "churn"))

customers$tariff_plan <- factor(customers$tariff_plan, levels = c(1, 2), labels = c("Pay as you go", "Contractual"))

customers$complains <- factor(customers$complains, levels = c(0, 1), labels = c("No complaint", "complaint"))

customers$status <- factor(customers$status, levels = c(1, 2), labels = c("Acitve", "Non-active"))

# Checking for missing values
# sum(is.na(customers))

second_snapshot <- customers |>
  sample_n(5)

colnames(second_snapshot) <- str_replace_all(colnames(second_snapshot), "_", " ")

colnames(second_snapshot) <- str_to_title(colnames(second_snapshot))

kable(
  second_snapshot |>
    select(1:6),
  caption = "A snapshot of the cleanded dataset"
) |>
  kable_styling(latex_options = c("striped", "hold_position"))

kable(
  second_snapshot |>
    select(7:12),
  caption = "A snapshot of the cleanded dataset"
) |>
  kable_styling(latex_options = c("striped", "hold_position"))

kable(
  second_snapshot |>
    select(13:14),
  caption = "A snapshot of the cleanded dataset"
) |>
  kable_styling(latex_options = c("striped", "hold_position"))
```



## Exploratory data analysis

- Identifying a criteria that distinguishes customers is our goal in this section.

Although we will not explore every avenue of the dataset, we will start by asking a few questions and then refine our questions as we dig deeper:

1.  What are the most and least common values within the data?
2.  Are there any unusual patterns?
3.  Are there any correlations within the data? Will start with calculated customer value in customers who left.



```{r include=FALSE}
customer_churn_value <- customers |>
  filter(churn == "churn")
```

```{r echo=FALSE, fig.cap="Distribution of Calculated Customer Value and Churn"}
ggplot(customer_churn_value, aes(x = customer_value)) +
  geom_histogram(bins = 30, color = "white") +
  labs(
    title = "Distribution of Customer Value in Churn Customers",
    x = "Customer Value",
    y = "Count"
  )
```

- Figure 1 shows that the most common values is zero which is not expected here it could be due to a missing value or real effect.
Something to notice here that the figure is skewed to the right which in return might not represent the customers as they are we will address this problem later.


- Next we will see the most common values in non-churn customers.

```{r echo=FALSE}
customer_non_churn_value <- customers |>
  filter(churn == "non-churn")

ggplot(customer_non_churn_value, aes(x = customer_value)) +
  geom_histogram(bins = 30, color = "white") +
  labs(
    title = "Distribution of Customer Value in Non-churn Customers",
    x = "Customer Value",
    y = "Count"
  )
```

- Figure 2 shows clustering or subgroups of customers with same values the most common value seems to be less than 500.

```{r, echo=FALSE, fig.cap="Average of using SMS between churn and non-churn", warning=FALSE}
# Visualizing the relation between age_group and distinct calls
ggplot(customers, aes(x = churn, y = frequency_of_sms)) +
  geom_boxplot(aes(fill = churn)) +
  stat_summary(fun = "mean", geom = "point", color = "red") +
  labs(
    title = "Mean Difference in Total Frequency of Using SMS",
    x = "Churn",
    y = "Frequency of Using SMS"
  ) +
  theme_classic()
```


```{r, echo=F, fig.cap="Average of using SMS and tariff plan", warning=F}
ggplot(customers, aes(x = tariff_plan, y = frequency_of_sms)) +
  geom_boxplot(aes(fill = tariff_plan)) +
  stat_summary(fun = "mean", geom = "point", color = "red") +
  labs(
    title = "Frequency of Using SMS and Tariff plan",
    x = "Frequency of Using SMS",
    y = "Tariff Plan"
  ) +
  theme_classic()
```



```{r, echo=F, fig.cap="Frequency of using SMS and age group", warning=F}
ggplot(customers, aes(x = age_group, y = frequency_of_sms)) +
  geom_boxplot(aes(fill = age_group)) +
  stat_summary(fun = "mean", geom = "point", color = "red") +
  labs(
    title = "Frequency of Using SMS and Age Group",
    x = "Frequency of Using SMS",
    y = "Age Group"
  ) +
  theme_classic()
```




```{r echo=F, warning=F, fig.cap="Distribution of Frequency SMS"}
ggplot(customers, aes(x = frequency_of_sms)) +
  geom_histogram(binwidth = 30, col = "white") +
  labs(
    title = "Distribution of Frequency of Using SMS",
    y = "Count",
    x = "Frequency of Using SMS"
  ) +
  coord_cartesian(xlim = c(0, 100))
```



- **Could there be a difference in SMS usage?**
Figure 3 compares the average SMS usage between churning and non-churning customers and shows a noticeable difference in average usage.

- **Tariff plan and SMS usage** 
In Figure 4, despite some overlap between plans in average SMS usage, there is still a difference, suggesting a potential link between plan and churn, but deeper analysis is required to determine if plan is a significant factor.

- **Age, usage and churn** As we have seen in Figure 5, there appears to be a relationship between usage frequency, SMS usage, age groups (as in this figure) and the decision of customers to either leave or stay.
Further analysis is needed here to confirm whether the company needs to improve its messaging system, tariff plans and perhaps tailor its marketing strategy more towards certain age groups.

- **Zero SMS usage?**. In Figure 6 we can see that the most common value is zero, which raises questions such as:

1.  Is it plausible that SMS usage is zero for most customers?
2.  The figure is skewed to the right, indicating non-normality, what could be the real value?

The zero usage of messages could happen for some customers, so dropping the values will not do much here.
But since the data is randomly selected, we can use bootstrapping to see if there is a meaningful difference in the statistical analysis part.




```{r, echo=FALSE, warning=FALSE}
# Getting summary stats
summ_stat_non_chrun <- customers |>
  filter(churn == "non-churn") |>
  select(where(is.numeric)) |>
  skim()

summ_stat_non_chrun <- summ_stat_non_chrun |>
  select(!c(skim_type, n_missing, complete_rate, numeric.hist))

colnames(summ_stat_non_chrun) <- str_replace(colnames(summ_stat_non_chrun), "skim_variable", "Variable Name")

colnames(summ_stat_non_chrun) <- str_remove_all(colnames(summ_stat_non_chrun), "numeric.")

summ_stat_chrun <- customers |>
  filter(churn == "churn") |>
  select(where(is.numeric)) |>
  skim()

summ_stat_chrun <- summ_stat_chrun |>
  select(!c(skim_type, n_missing, complete_rate, numeric.hist))

colnames(summ_stat_chrun) <- str_replace(colnames(summ_stat_chrun), "skim_variable", "Variable Name")

colnames(summ_stat_chrun) <- str_remove_all(colnames(summ_stat_chrun), "numeric.")



cor_matrix <- customers |>
  group_by(churn) |>
  summarize(cor = cor(charge_amount, frequency_of_use))



kable(summ_stat_non_chrun, digits = 2, caption = "Summary statistics for non-churn customers") |>
  kable_styling(latex_options = c("striped", "hold_position"))

kable(summ_stat_chrun, digits = 2, caption = "Summary statistics for churn customers") |>
  kable_styling(latex_options = c("striped", "hold_position"))

kable(cor_matrix, , digits = 2, caption = "The correlation between frequecy of use and charge amount") |>
  kable_styling(latex_options = c("striped", "hold_position"))
```




- The summary in table 5 and 6 shows a difference in means between customers who stayed in the company for example the mean for frequency of using **SMS** is higher in customers who stayed.



## Statistical analysis

- **Assessing the significance in differences**

- Conducting hypothesis test here compare the difference, we set our level of rejection (i.e meaning that the risk of rejecting the null hypothesis when it is true) to be 5%.

Figure 3 illustrates the difference in frequency of SMS usage between churn and non churn customers.Before drawing any conclusions, we will check if the difference is statistically discernible (i.e significant).

As this will inform us about the customer behavior.

**The Testing Framework** :

1. **Null hypothesis**{$H_0$} There is no difference in mean of frequency of SMS usage between churn and non churn customers.

2. **Alternative hypothesis**{$H_0$} There is a difference in mean of SMS usage between the churn and non churn customers.

```{r, echo=FALSE, warning=FALSE}
set.seed(123)

# calculate the observed statistic
observed_statistic <- customers |>
  specify(frequency_of_sms ~ churn) |>
  calculate(stat = "diff in means", order = c("non-churn", "churn"))

null_distn <- customers |>
  specify(frequency_of_sms ~ churn) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "diff in means", order = c("non-churn", "churn"))

null_distn |>
  visualize() +
  shade_p_value(obs = observed_statistic, direction = "both")

# Get p-value
pvalue1 <- null_distn |>
  get_pvalue(obs_stat = observed_statistic, direction = "both")


```

Here the the p-value `r pvalue1$p_value` that i.e (the compatibility of data and the null hypothesis), we can conclude that we have a convincing evidence to reject the null hypothesis.

Indicating that there are real differences between churn and non-churn in frequency of SMS usage.

-   To quantify the difference, we will construct a confidence interval for the difference in **SMS usage** :

```{r, echo=FALSE, warning=FALSE}
set.seed(123)
t_hat <- customers |>
  specify(frequency_of_sms ~ churn) |>
  calculate(stat = "t", order = c("non-churn", "churn"))

boot_dist <- customers |>
  specify(frequency_of_sms ~ churn) |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "t", order = c("non-churn", "churn"))

standard_error_ci <- boot_dist |>
  get_ci(type = "se", point_estimate = t_hat)

visualize(boot_dist) +
  shade_confidence_interval(endpoints = standard_error_ci)

kable(standard_error_ci, digits = 2, caption = "95% level of confidence in the range of differences in churn and non-churn customers")
```

- **Assessing the Differences in Tariff Plans**

Figure 4 is showing a difference in frequency of SMS usage between tariff plans before continuing we will check if the difference is statistically discernible (i.e significant).

To do so we will conduct a hypothesis test :

1.  **Null hypothesis**{$H_0$} There is no difference in mean of frequency of SMS usage between the two tariff plans.

2.  **Alternative hypothesis**{$H_A$} There is a difference in mean of SMS usage between the two tariff plans.

```{r, echo=FALSE, warning=FALSE}
set.seed(123)

# calculate the observed statistic
observed_statistic <- customers |>
  specify(frequency_of_sms ~ tariff_plan) |>
  calculate(stat = "diff in means", order = c("Contractual", "Pay as you go"))

null_distn <- customers |>
  specify(frequency_of_sms ~ tariff_plan) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "diff in means", order = c("Contractual", "Pay as you go"))

null_distn |>
  visualize() +
  shade_p_value(obs = observed_statistic, direction = "both")

# Get p-value
pvalue <- null_distn |>
  get_pvalue(obs_stat = observed_statistic, direction = "both")


```

-   Since the p-value is `r pvalue$p_value`, we can say that we have a convincing evidence to reject the null hypothesis.

Indicating that there are real differences between tariff plans in frequency of SMS usage.

An implication for the company might be asking what can make their messaging system more attractive.

Even though there are limitation to this analysis that needs to be considered, we will discuss it later.

## Predictive analytics

Knowing in advance which group or subgroup of customers are likely to leave, is our aim here in which we are going to build a model step by step :

1. Fit the model: We use logistic regression for the predictive model and apply backward selection to choose variables with significant p-values.

```{r, echo=FALSE, warning=FALSE}
set.seed(123)

# Fit a model by using backward selection of variables
glm.fits <- glm(churn ~ call_failure + charge_amount + frequency_of_sms + frequency_of_use, frequency_of_sms, data = customers, family = binomial)

# Putting the model in a tidy format
model_tidy <- glm.fits |>
  tidy(conf.int = TRUE) |>
  mutate_if(is.numeric, round, digits = 2) |>
  clean_names()

kable(model_tidy, caption = "A table for logistic regression based-model") |>
  kable_styling(latex_options = c("striped", "hold_position"))
```



- Table 9 summarizes the logistic regression model after backward selection, showing only the variables with significant p-values.

2.For the logistic regression model to provide valid results, certain assumptions must be satisfied :

**Independence**: The data has been collected randomly, so we assume sufficiency in this condition.

**Linearity**: Which for this condition to work linear relationship between logit and predictor variables needs to exist:


```{r, echo=F, warning=F}
set.seed(123)

# Check the model assumptions
customers$fitted_probs <- predict(glm.fits, type = "response")

# Create buckets for fitted probabilities
customers$bucket <- cut(customers$fitted_probs,
  breaks = seq(0, 1, by = 0.1),
  include.lowest = TRUE
)

# Calculate Average Predicted Probability
bucket_summary <- customers |>
  group_by(bucket) |>
  summarize(
    avg_predicted = mean(fitted_probs),
    observed = mean(as.numeric(churn) - 1),
    n = n()
  )

#  Calculate the Observed Probability and 95% Confidence Interval
bucket_summary <- bucket_summary |>
  mutate(
    se = sqrt((observed * (1 - observed)) / n),
    lower_ci = pmax(0, observed - 1.96 * se), # Lower bound
    upper_ci = pmin(1, observed + 1.96 * se)
  ) # Upper bound

# Plot the results
ggplot(bucket_summary, aes(x = avg_predicted, y = observed)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.02) +
  labs(x = "Average Predicted Probability", y = "Observed Probability") +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  theme_classic()
```




The plot compares predicted probabilities (x-axis) with observed probabilities (y-axis). The points should cluster around the red diagonal line (representing perfect predictions). This indicates that the model’s linearity assumption holds since the observed probabilities match predicted values closely.


3. After checking the conditions, we check the accuracy of the model (i.e how much the model can explain the churn variable).

To do so we will use 5-fold cross-validation

```{r, echo=FALSE}
set.seed(123)

# Check the model accuracy
train_control <- trainControl(method = "cv", number = 5)

model <- train(churn ~ call_failure + charge_amount + frequency_of_sms + frequency_of_use,
  data = customers,
  method = "glm",
  family = binomial,
  trControl = train_control
)

results <- round(model$results$Accuracy, 2)
```

The accuracy of the model is **`r results`**.

- We can evaluate the model in another way using confusion matrix :

```{r echo=FALSE}
set.seed(123)

# Initialize predictions as "No Churn"
customers$predicted_churn <- rep("non-churn", nrow(customers))

# Replace "No Churn" with "Churn" for probabilities > 0.5
customers$predicted_churn[customers$fitted_probs > 0.5] <- "churn"

matrix <- table(customers$predicted_churn, customers$churn)

kable(matrix, caption = "Confusion matrix")

mean_perd <- round(mean(customers$predicted_churn == customers$churn), 2)
```



The table there is telling true negatives, positives, and false negatives and positives(i.e how many predictions that our model got it right and it was about `r mean_perd`).

But this accuracy might be **misleading** because the model was trained and tested on the same data set.

We will take the same steps but training and testing on different data sets.

```{r, echo=FALSE}
set.seed(123)

# Create training data set
train_data <- customers$age < 30

customers.30 <- customers[!train_data, ]

# Fit the model
glm.fits2 <- glm(churn ~ call_failure + charge_amount + frequency_of_sms + frequency_of_use, frequency_of_sms, data = customers, family = binomial, subset = train_data)

glm.probs <- predict(glm.fits2, customers.30, type = "response")


# Initialize predictions as "No Churn"
glm.pred <- rep("non-churn", nrow(customers.30))

# Replace "No Churn" with "Churn" for probabilities > 0.5
glm.pred[glm.probs > 0.5] <- "churn"

matrix1 <- table(glm.pred, customers.30$churn)

kable(matrix1, caption = "More accurate evaluation on training and testing set")

# Accuracy rate
accuracy <- round(mean(glm.pred == customers.30$churn), 2)

# Miss-classification rate
Missclassification_rate <- round(mean(glm.pred != customers.30$churn), 2)
```


The accuracy rate here is `r accuracy` and Miss-classification rate is `r Missclassification_rate`.

4. Model predictions. We plug in some values to the model and get the probability of churning :

For example when (call failure is 8, 10), (charge_amount 1, 3), (frequency of sms 10, 14) and (frequency_of_use 82, 44) :

```{r echo=FALSE, warning=FALSE}
set.seed(123)
predic_churn <-
  round(predict(glm.fits,
    newdata = data.frame(call_failure = c(8, 10), charge_amount = c(1, 3), frequency_of_sms = c(10, 14), frequency_of_use = c(82, 44)),
    type = "response"
  ), 2)

kable(predic_churn, caption = "Probability of churning for certain customers")
```



[**As the task at hand is only to do prediction and not to interpret coefficients, a model that suffers from high multicollinearity will likely lead to unbiased predictions of the response variable. So multicollinearity is likely to not cause any substantial problems.**](https://openintro-ims.netlify.app/inf-model-mlr)

But we should be careful about **extrapolations**. In other words, just because our model supports a linear relationship doesn’t mean that relationship holds for values outside our range. Predictions for such values is far away from the actual range often aren’t accurate.

## Conclusion

Our model, with an accuracy rate of 0.85, offers valuable predictive power, meaning the company can identify customers at risk of churn and intervene with tailored retention strategies. While some miss-classifications occurred, the model still provides strong actionable insights.


# Limitations 

-  **Observational Data**: Since this is an observational study, causality cannot be established. While the model can predict the likelihood of churn, it does not identify the exact causes behind customer decisions to leave. This means any results should be interpreted as a correlation, not causation.


- **Potential Biases**: The data is limited to one telecom company, we may not be able to generalize to other sectors or regions.

# Future improvment

1. Experimentation for Causal Inference.

2. Considering more external Factors.
